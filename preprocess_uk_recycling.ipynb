{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create virtual env for jupyter notebook\n",
    "# conda create --no-default-packages -n waste python=3.6 pandas numpy scipy scikit-learn matplotlib\n",
    "# conda install --name waste --copy -c conda-forge folium\n",
    "# source activate waste\n",
    "# # sankey charts work in jupyter notebook but not in jupyter lab\n",
    "# pip install floweaver\n",
    "# pip install jupyterlab\n",
    "# pip install ipysankeywidget\n",
    "# jupyter nbextension enable --py --sys-prefix ipysankeywidget\n",
    "# pip install --user floweaver ipysankeywidget\n",
    "# jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "# python -m ipykernel install --user --name waste --display-name \"UK waste flows 3.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Get data\n",
    " \n",
    " http://www.wastedataflow.org/reports/default.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "import floweaver\n",
    "\n",
    "# pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('./datasets/Raw_Data_Download_London_quarter.xlsx', sheet_name='Q100', skiprows=0, header=1)\n",
    "df2 = pd.read_excel('./datasets/2019/Raw Data Download.xlsx', sheet_name='Q100', skiprows=0, header=1)\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['Period'].str.split(' ').str[1]\n",
    "df['MaterialGroup'] = df['MaterialGroup'].replace(np.nan,'Unknown',regex=True).replace('','Unknown',regex=True)\n",
    "df['FacilityType'] = df['FacilityType'].replace(np.nan,'Unknown',regex=True).replace('','Unknown',regex=True)\n",
    "df['Material'] = df['Material'].replace(np.nan,'Unknown',regex=True).replace('','Unknown',regex=True)\n",
    "df['WasteStreamType'] = df['WasteStreamType'].replace(np.nan,'Unknown',regex=True).replace('','Unknown',regex=True)\n",
    "df['OutputProcessType'] = df['OutputProcessType'].replace(np.nan,'Unknown',regex=True).replace('','Unknown',regex=True)\n",
    "\n",
    "df['FacilityPostCode'] = df['FacilityPostCode'].replace(np.nan,'',regex=True)\n",
    "df['FacilityName'] = df['FacilityName'].replace(np.nan,'',regex=True)\n",
    "df['FacilityAddress'] = df['FacilityAddress'].replace(np.nan,'',regex=True)\n",
    "\n",
    "# manually add in Waste Management Authority for London Boroughs which share waste management services\n",
    "wma = {'North London Waste Authority': \n",
    "           ['North London Waste Authority', 'Barnet LB', 'Camden LB', 'Enfield LB', 'Hackney LB', 'Haringey LB', 'Islington LB', 'Waltham Forest LB'], \n",
    "       'Western Riverside Waste Authority': \n",
    "           ['Western Riverside Waste Authority', 'Hammersmith and Fulham LB', 'Royal Borough of Kensington and Chelsea', 'Lambeth LB', 'Wandsworth LB'], \n",
    "       'West London Waste Authority': \n",
    "           ['West London Waste Authority', 'Brent LB', 'Ealing LB', 'Harrow LB', 'Hillingdon LB', 'Hounslow LB', 'Richmond upon Thames LB'], \n",
    "       'South London Waste Partnership': \n",
    "           ['South London Waste Partnership', 'Croydon LB', 'Royal Borough of Kingston upon Thames ', 'Merton LB', 'Sutton LB'], \n",
    "       'East London Waste Authority': \n",
    "           ['East London Waste Authority', 'Barking and Dagenham LB', 'Havering LB', 'Newham LB', 'Redbridge LB']\n",
    "      }\n",
    "    \n",
    "    \n",
    "wma_reverse = {}\n",
    "for k,v in wma.items():\n",
    "    for vl in v:\n",
    "        if vl not in wma_reverse.keys():\n",
    "            wma_reverse[vl] = k\n",
    "        else:\n",
    "            wma_reverse[vl].add(set(k))\n",
    "\n",
    "df['WasteManagementAuthority'] = df[['Authority']].apply(lambda x: wma_reverse[x[0]] if x[0] in wma_reverse.keys() else x[0], axis =1)\n",
    "\n",
    "def get_quarter(year, period):\n",
    "    m = period.split(' ')[0]\n",
    "    if m == 'Jan':\n",
    "        return int(year + '1')\n",
    "    elif m == 'Apr':\n",
    "        return int(year + '2')\n",
    "    elif m == 'Jul':\n",
    "        return int(year + '3')\n",
    "    elif m == 'Oct':\n",
    "        return int(year + '4')\n",
    "    else:\n",
    "        return int(year + '5')\n",
    "    \n",
    "# Create integer quarter - so that could filter flows on this property (quarter greater than)                 \n",
    "df['Quarter'] = df[['Period', 'year']].astype(str).apply(lambda row: get_quarter(row[1], row[0]), axis = 1)\n",
    "\n",
    "# TotalTonnes matches the sum of TonnesByMaterial across materials within same processing facility,\n",
    "# with the same figure repeated across several material lines\n",
    "# we use TonnesBy Material when material is known and TotalTonnes when Material is Unknown or it's a Process Loss\n",
    "df['Tonnes'] = df[['Material', 'TotalTonnes', 'TonnesByMaterial']].apply(lambda row: row[1] if row[0] in [\"Unknown\"] else row[2], axis = 1)\n",
    "\n",
    "# unique id of row for duplication checks and defining flow id\n",
    "#df['rownum'] = pd.Series(np.arange(len(df))).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocode facility locations \n",
    "\n",
    "There are no unique ids for facilities, facility names and addresses are manually entered with many errors\n",
    "\n",
    "Geocoding them with google maps api to get unique id and location coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of facility names and addresses to create a lookup table\n",
    "# facility_addresses = df[(df['FacilityAddress']!='') | (df['FacilityName']!='') | (df['FacilityPostCode']!='')][\n",
    "#     ['NationalFacilityId', 'FacilityName','FacilityAddress', 'FacilityPostCode']\n",
    "#     ].groupby(['FacilityName','FacilityAddress', 'FacilityPostCode']).count().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_location = {}\n",
    "\n",
    "# for loc in locations:\n",
    "#     params = {\n",
    "#     'address': loc,\n",
    "#     'key' : api_key    \n",
    "#     }\n",
    "#     req = requests.get(GOOGLE_MAPS_API_URL, params=params)\n",
    "#     res = req.json()\n",
    "    \n",
    "#     if res['status'] == 'ZERO_RESULTS':\n",
    "#         geodata = []\n",
    "#         # put unknown into atlantic ocean\n",
    "#         geodata.append(14.5994)\n",
    "#         geodata.append(28.6731)\n",
    "#         geodata.append('location unknown')\n",
    "#         geodata.append('unknown')\n",
    " \n",
    "#     else:    \n",
    "#         result = res['results'][0]\n",
    "#         geodata = []\n",
    "#         geodata.append(result['geometry']['location']['lat'])\n",
    "#         geodata.append(result['geometry']['location']['lng'])\n",
    "#         geodata.append(result['formatted_address'])\n",
    "#         geodata.append(result['place_id'])\n",
    "\n",
    "    \n",
    "#     geo_location[loc] = geodata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./datasets/geocoded.json', 'w') as fp:\n",
    "#     json.dump(geo_location, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/geocoded.json', 'r') as f:\n",
    "    geo_location = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'] = df[['FacilityName','FacilityAddress', 'FacilityPostCode']].apply(lambda x: \n",
    "                                                         str(x[1]) + ', ' + str(x[2]) if x[0] == 'Other/Exempt' \n",
    "                                                        else str(x[0]) + ', ' + str(x[1]) + ', ' + str(x[2])\n",
    "                                                         , axis =1)\n",
    "\n",
    "df['Location_Coordinates'] = df['address'].apply(lambda x: [geo_location[x][0],geo_location[x][1]] if x != ', , ' else [14.5994,28.6731])\n",
    "df['FacilityAddressGeo'] = df['address'].apply(lambda x: geo_location[x][2] if x != ', , ' else '')\n",
    "df['FacilityIdGeo'] = df['address'].apply(lambda x: geo_location[x][3] if x != ', , ' else '')\n",
    "df['FacilityIdGenerated'] = df[['NationalFacilityId','FacilityTypeId','FacilityIdGeo']].apply(lambda x: str(x[0]) + \"_\" + str(x[1]) + \"_\" + str(x[2]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial houshold waste streams at source (collected by Local Authority)\n",
    "df_stream_same = df[ ((df['WasteProcessorOutputId']==0)&(df['SenderWasteProcessorOutputId']==0))  \n",
    "                    & ((df['MaterialId']!=0)\n",
    "                        | ( (df['FacilityType'] =='Advanced thermal treatment') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Anaerobic or Aerobic Digestion Segregated') & (df['WasteStreamType'] == 'Food waste')) \n",
    "                        | ( (df['FacilityType'] =='Anaerobic or Aerobic Digestion Segregated') & (df['WasteStreamType'] == 'Green waste')) \n",
    "                        | ( (df['FacilityType'] =='Anaerobic or Aerobic Digestion Segregated') & (df['WasteStreamType'] == 'Mixed green and food waste')) \n",
    "                        | ( (df['FacilityType'] =='Exporter - recycling (qu19)') & (df['WasteStreamType'] == 'Source segregated recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Exporter - reuse (qu35)') & (df['WasteStreamType'] == 'Source segregated recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Hazardous landfill') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='In vessel composting') & (df['WasteStreamType'] == 'Food waste')) \n",
    "                        | ( (df['FacilityType'] =='In vessel composting') & (df['WasteStreamType'] == 'Green waste')) \n",
    "                        | ( (df['FacilityType'] =='In vessel composting') & (df['WasteStreamType'] == 'Mixed green and food waste')) \n",
    "                        | ( (df['FacilityType'] =='Incineration with energy recovery') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Incineration with energy recovery') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Incineration without energy recovery') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Inert landfill') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Inert landfill') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Material From WDA') & (df['WasteStreamType'] == 'Comingled recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Material From WDA') & (df['WasteStreamType'] == 'Mixed green and food waste')) \n",
    "                        | ( (df['FacilityType'] =='Material From WDA') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Material From WDA') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Material From WDA') & (df['WasteStreamType'] == 'Source segregated recyclate')) \n",
    "#                         | ( (df['FacilityType'] =='Materials recovery facility') & (df['WasteStreamType'] == 'Comingled recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Materials recovery facility') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Mechanical Biological treatment') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Non-hazardous landfill') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Non-hazardous landfill') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Other Method') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='RDF, autoclave, MHT or similar') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Reprocessor - recycling (qu19)') & (df['WasteStreamType'] == 'Green waste')) \n",
    "                        | ( (df['FacilityType'] =='Reprocessor - recycling (qu19)') & (df['WasteStreamType'] == 'No Waste Stream Type')) \n",
    "                        | ( (df['FacilityType'] =='Reprocessor - recycling (qu19)') & (df['WasteStreamType'] == 'Source segregated recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Residual waste MRF') & (df['WasteStreamType'] == 'Residual waste')) \n",
    "                        | ( (df['FacilityType'] =='Reuse (qu35)') & (df['WasteStreamType'] == 'Source segregated recyclate')) \n",
    "                        | ( (df['FacilityType'] =='Windrow or other composting') & (df['WasteStreamType'] == 'Food waste')) \n",
    "                        | ( (df['FacilityType'] =='Windrow or other composting') & (df['WasteStreamType'] == 'Green waste')) \n",
    "                        | ( (df['FacilityType'] =='Windrow or other composting') & (df['WasteStreamType'] == 'Mixed green and food waste')) \n",
    "                    )][[\n",
    "                'WasteProcessorOutputId', 'SenderWasteProcessorOutputId',\n",
    "                'WasteProcessorId', 'WasteStreamId',  \n",
    "                'Authority', 'AuthorityId', 'WasteManagementAuthority',\n",
    "                'Quarter',\n",
    "                \"FacilityName\", \"FacilityName\", \"NationalFacilityId\", \"Location_Coordinates\", \"FacilityAddressGeo\",\n",
    "                \"FacilityType\", \"FacilityType\",\"FacilityTypeId\",\n",
    "                'WasteStreamTypeId', 'WasteStreamType',\n",
    "                \"OutputProcessType\", \"OutputProcessTypeId\",\n",
    "                'MaterialId', 'Material', 'MaterialGroup', \n",
    "                'Tonnes',\n",
    "                \"FacilityIdGenerated\", \"FacilityIdGenerated\", 'FacilityIdGenerated'\n",
    "                ]]\n",
    "\n",
    "df_stream_same.columns = ['WasteProcessorOutputId', 'SenderWasteProcessorOutputId',\n",
    "                'WasteProcessorId', 'WasteStreamId',\n",
    "                'Authority', 'AuthorityId', 'WasteManagementAuthority',\n",
    "                'Quarter', \n",
    "                \"FacilityName_sender\", \"FacilityName\", \"NationalFacilityId\", \"Location_Coordinates\", \"FacilityAddressGeo\",\n",
    "                \"FacilityType_sender\", \"FacilityType\",\"FacilityTypeId\",\n",
    "                'WasteStreamTypeId', 'WasteStreamType',\n",
    "                \"OutputProcessType\", \"OutputProcessTypeId\",\n",
    "                'MaterialId', 'Material', 'MaterialGroup',\n",
    "                'Tonnes',\n",
    "                \"sender_facility\",\"processing_facility\", 'FacilityIdGenerated'\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of edges - subsequent waste streams from Local Authority to Processing Facilities and from Facilities to each other or export\n",
    "\n",
    "# Pick up sender facility info from another row of dataset, join on 'WasteProcessorOutputId' = 'SenderWasteProcessorOutputId'\n",
    "# (note that source can be 0 but destination can't - exclude these)  \n",
    "\n",
    "# Source facility\n",
    "df_Source_Facility = df[[\n",
    "                'WasteProcessorOutputId', 'SenderWasteProcessorOutputId',\n",
    "                'WasteProcessorId', 'WasteStreamId',  \n",
    "                'AuthorityId',\n",
    "                'Quarter',\n",
    "                'WasteStreamTypeId',\n",
    "                \"FacilityType\", \"FacilityName\",\n",
    "                \"FacilityIdGenerated\"]]\n",
    "\n",
    "df_Source_Facility.columns = ['key', 'SenderWasteProcessorOutputId_source',\n",
    "                'WasteProcessorId', 'WasteStreamId',  \n",
    "                'AuthorityId',\n",
    "                'Quarter',\n",
    "                'WasteStreamTypeId',\n",
    "                \"FacilityType_sender\", \"FacilityName_sender\",\n",
    "                \"sender_facility\"]\n",
    "\n",
    "\n",
    "# Destination Facility \n",
    "# Filter out Final Destination which would cause duplicates through key being same for different materials)\n",
    "# (an alternative would be to join the final destination separately using material id \n",
    "# because apparently tonned by material can be diferent in final destination line )                      \n",
    "df_Destination_Facility = df[~((df['WasteProcessorOutputId']==0)&(df['SenderWasteProcessorOutputId'] ==0) |\n",
    "                               (df['FacilityTypeId'] == 22)\n",
    "                              )][[\n",
    "                'WasteProcessorOutputId', 'SenderWasteProcessorOutputId',\n",
    "                'WasteProcessorId', 'WasteStreamId',  \n",
    "                'Authority', 'AuthorityId', 'WasteManagementAuthority',\n",
    "                'Quarter',\n",
    "                \"FacilityName\", \"NationalFacilityId\", \"Location_Coordinates\", \"FacilityAddressGeo\",\n",
    "                \"FacilityType\",\"FacilityTypeId\",\n",
    "                'WasteStreamTypeId', 'WasteStreamType',\n",
    "                \"OutputProcessType\", \"OutputProcessTypeId\",\n",
    "                'MaterialId', 'Material', 'MaterialGroup', \n",
    "                'Tonnes',\n",
    "                \"FacilityIdGenerated\", 'FacilityIdGenerated']]\n",
    "\n",
    "df_Destination_Facility.columns = [\n",
    "                'WasteProcessorOutputId', 'key',\n",
    "                'WasteProcessorId', 'WasteStreamId',  \n",
    "                'Authority', 'AuthorityId', 'WasteManagementAuthority',\n",
    "                'Quarter',\n",
    "                \"FacilityName\", \"NationalFacilityId\", \"Location_Coordinates\", \"FacilityAddressGeo\",\n",
    "                \"FacilityType\",\"FacilityTypeId\",\n",
    "                'WasteStreamTypeId', 'WasteStreamType',\n",
    "                \"OutputProcessType\", \"OutputProcessTypeId\",\n",
    "                'MaterialId', 'Material', 'MaterialGroup', \n",
    "                'Tonnes',\n",
    "                \"processing_facility\", 'FacilityIdGenerated']\n",
    "\n",
    "\n",
    "                 \n",
    "# Create Edges with both source and destination, excluding rows where material is unknown and final destinations           \n",
    "df_edges = pd.merge(df_Source_Facility\n",
    "                     , df_Destination_Facility\n",
    "                     , how='inner' \n",
    "                     , on=['key', 'WasteProcessorId', 'WasteStreamId', \n",
    "                          'AuthorityId', 'Quarter','WasteStreamTypeId']\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.concat([df_stream_same[['Quarter', 'AuthorityId', 'Authority',\n",
    "       'WasteManagementAuthority', \"FacilityName_sender\", 'FacilityName',\n",
    "       'Location_Coordinates', 'FacilityAddressGeo', \"FacilityType_sender\",'FacilityType', \n",
    "       'FacilityTypeId', 'WasteStreamTypeId', 'WasteStreamType', 'OutputProcessType',\n",
    "       'OutputProcessTypeId', 'MaterialId', 'Material', 'MaterialGroup',\n",
    "       'Tonnes', 'sender_facility', 'processing_facility', 'FacilityIdGenerated']]\n",
    "                      , df_edges[[ 'Quarter', 'AuthorityId', 'Authority',\n",
    "       'WasteManagementAuthority', \"FacilityName_sender\", 'FacilityName',\n",
    "       'Location_Coordinates', 'FacilityAddressGeo', \"FacilityType_sender\", 'FacilityType',  \n",
    "       'FacilityTypeId', 'WasteStreamTypeId','WasteStreamType', 'OutputProcessType',\n",
    "       'OutputProcessTypeId', 'MaterialId', 'Material', 'MaterialGroup',\n",
    "       'Tonnes', 'sender_facility',  'processing_facility', 'FacilityIdGenerated']]]\n",
    "                     , sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['rownum'] = pd.Series(np.arange(len(df_graph))).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"material type\" called \"landfill\" becasue it's a kind of final destination\n",
    "# When facility type is incineration and no material extrated (i.e. burned off), define material as \"process loss\"\n",
    "# Don't want to create a system boundary because want to compare tonnes of recovered matrials vs landfilled and incinerated (vanishing) tonnes\n",
    "\n",
    "df_graph['MaterialGroup'] = df_graph[['MaterialGroup','FacilityType']].apply(lambda row: row[1] if row[1] in ['Inert landfill', 'Non-hazardous landfill', 'Hazardous landfill'] else row[0], axis=1)\n",
    "\n",
    "df_graph['MaterialGroup'] = df_graph[['MaterialGroup','FacilityType_sender','FacilityType','OutputProcessType']].apply(lambda row: \n",
    "                                    row[2] if ((row[1] != row[2]) \n",
    "                                               & (row[2] in ['Inert landfill', 'Non-hazardous landfill', 'Hazardous landfill'])\n",
    "                                               & (row[1] in ['Incineration with energy recovery','RDF, autoclave, MHT or similar','Incineration without energy recovery','Material From WDA', 'Mechanical Biological treatment', 'In vessel composting', 'Incineration with energy recovery', 'Anaerobic or Aerobic Digestion Whole Waste', 'Anaerobic or Aerobic Digestion Segregated']) \n",
    "#                                                & (row[0] in ['Unknown','Inert landfill', 'Non-hazardous landfill', 'Hazardous landfill']) \n",
    "                                               & (row[3] == 'Contamination (process rejects)')) \n",
    "                                    else row[3] if ((row[2] in ['Incineration with energy recovery','RDF, autoclave, MHT or similar','Incineration without energy recovery','Material From WDA', 'Mechanical Biological treatment']) \n",
    "                                                    & (row[0] == 'Unknown') \n",
    "                                                    & (row[3] in ['Process loss', 'Incinerator bottom ash', 'Incinerator fly ash'])) \n",
    "                                    else 'Process loss' if ((row[2] in ['Incineration with energy recovery','RDF, autoclave, MHT or similar','Incineration without energy recovery','Material From WDA', 'Mechanical Biological treatment']) \n",
    "                                                            & (row[0] == 'Unknown') \n",
    "                                                            & (row[3] == 'Unknown')) \n",
    "                                    else row[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_graph.to_csv('./datasets/waste_graph.csv', encoding=\"utf8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UK waste flows 3.6",
   "language": "python",
   "name": "waste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
